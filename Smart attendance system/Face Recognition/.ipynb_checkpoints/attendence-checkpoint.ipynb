{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Attendance System\n",
    "\n",
    "Nowadays there are many ways of authenticating yourself, like using password, retina scan, fingerprint etc. Face can also be used for this purpose. In this notebook we will make a face recognition system using Siamese network.\n",
    "This is different from face verification where the task is to know whether given two input images are same or not.\n",
    "Here the task is see whether the given input image is of any person who is registered with the system or not. There can be multiple users registered with the system.\n",
    "\n",
    "The advantage of **Siamese Network** is that it allows a way to do this sort of verification task with very little user data, as it is quite unreasonable to train using thousands of images for each user. Here we will be using **FaceNet Model**.\n",
    "\n",
    "FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. So by comparing two such vectors, we can then determine if two pictures are of the same person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "K.set_image_data_format('channels_first')\n",
    "import urllib\n",
    "import pickle\n",
    "import cv2\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pymysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-37180a0e061a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minception_blocks_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymysql\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'pymysql'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utility import *\n",
    "from webcam_utility import *\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "import pymysql as pms\n",
    "import time\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The model makes an encoding vector consisting of 128 numbers for the input image. Two encodings are compared and if the two encodings are similar then we say that the two images are of the same person otherwise they are different. \n",
    "The model uses **Triplet loss function**. The aim is to minimize this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "The model outputs a vector of 128 numbers which represent encoding for the given input image. We will be using this encoding vector for comparing two images.\n",
    "#### Input\n",
    "- This network takes as input 96x96 RGB image as its input. Specifically, inputs a tensor of shape $(m, n_C, n_H, n_W)$ , where $n_C$ = channel.\n",
    "\n",
    "\n",
    "#### Output\n",
    "- A matrix of shape **(m, 128)** where the 128 numbers are the encoding values for $ith$ image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)\n",
    "\n",
    "#os.makedirs(os.path.dirname('database\\\\user_dict.pickle'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Database\n",
    "\n",
    "We will create a database of registered. For this we will use a simple dictionary and map each registered user with his/her face encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the user database\n",
    "def ini_user_database():\n",
    "    # check for existing database\n",
    "    if os.path.exists('database/user_dict.pickle'):\n",
    "        with open('database/user_dict.pickle', 'rb') as handle:\n",
    "            user_db = pickle.load(handle)   \n",
    "    else:\n",
    "        user_db = {}    \n",
    "    return user_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a new user face to the database using his/her image stored on disk using the image path\n",
    "def add_user_img_path(user_db, FRmodel, name, img_path):\n",
    "    if name not in user_db: \n",
    "        resize_img(img_path)\n",
    "        user_db[name] = img_to_encoding(img_path, FRmodel)\n",
    "        # save the database\n",
    "        with open('database\\\\user_dict.pickle', 'wb') as handle:\n",
    "                pickle.dump(user_db, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('User ' + name + ' added successfully')\n",
    "    else:\n",
    "        print('The name is already registered! Try a different name.........')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a new user using image taken from webcam\n",
    "def add_user_webcam(user_db, FRmodel, name):\n",
    "    # we can use the webcam to capture the user image then get it recognized\n",
    "    face_found = detect_face(user_db, FRmodel)\n",
    "\n",
    "    if face_found:\n",
    "        resize_img(\"saved_image\\\\1.jpg\")\n",
    "        if name not in user_db: \n",
    "            add_user_img_path(user_db, FRmodel, name, \"saved_image\\\\1.jpg\")\n",
    "        else:\n",
    "            user_db[name] = img_to_encoding(\"saved_image\\\\1.jpg\", FRmodel)\n",
    "            # save the database\n",
    "            with open('database\\\\user_dict.pickle', 'wb') as handle:\n",
    "                    pickle.dump(user_db, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('User ' + name + ' added successfully')\n",
    "    else:\n",
    "        print('There was no face found in the visible frame. Try again...........')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes a registered user from database\n",
    "def delete_user(user_db, name):\n",
    "    popped = user_db.pop(name, None)\n",
    "    \n",
    "    if popped is not None:\n",
    "        print('User ' + name + ' deleted successfully')\n",
    "        # save the database\n",
    "        with open('database\\\\user_dict.pickle', 'wb') as handle:\n",
    "                pickle.dump(user_db, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    elif popped == None:\n",
    "        print('No such user !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    #Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    resize_img(image_path)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    ## Find the closest encoding ##\n",
    "    min_dist = 100\n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add or delete user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a dict for keeping track of mapping of each person with his/her face encoding\n",
    "user_db = ini_user_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_user_img_path(user_db, FRmodel, \"Rohith K\", \"images/self_study_pictures/1.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Nischal\", \"images/self_study_pictures/2.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Jeelani\", \"images/self_study_pictures/3.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Ajith\", \"images/self_study_pictures/4.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Manjunath\", \"images/self_study_pictures/5.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Shashank\", \"images/self_study_pictures/6.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Sachin CW\", \"images/self_study_pictures/7.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Sachin RD\", \"images/self_study_pictures/8.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Shalu\", \"images/self_study_pictures/9.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Hemanth\", \"images/self_study_pictures/10.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Repana\", \"images/self_study_pictures/11.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Rohith Raj\", \"images/self_study_pictures/12.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Sanjana\", \"images/self_study_pictures/13.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Ranjith\", \"images/self_study_pictures/14.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Praveen\", \"images/self_study_pictures/15.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Sahana\", \"images/self_study_pictures/16.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Ujwala\", \"images/self_study_pictures/17.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Mamatha\", \"images/self_study_pictures/18.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Pooja\", \"images/self_study_pictures/19.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Saleem\", \"images/self_study_pictures/20.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Sharanya\", \"images/self_study_pictures/21.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Raghavendra\", \"images/self_study_pictures/22.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Rakshith\", \"images/self_study_pictures/23.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Sanket\", \"images/self_study_pictures/24.jpg\")\n",
    "add_user_img_path(user_db, FRmodel, \"Salman\", \"images/self_study_pictures/25.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's Rakshith, the distance is 0.073503986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.073503986, 'Rakshith')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/self_study_pictures/archive/test.jpg\", user_db, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Ranjith deleted successfully\n"
     ]
    }
   ],
   "source": [
    "delete_user(user_db,\"Ranjith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Rohith Rao added successfully\n"
     ]
    }
   ],
   "source": [
    "add_user_webcam(user_db, FRmodel, \"Rohith Rao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** Enter \"q\" to quit **********************\n",
      "Welcome Rakshith!\n",
      "distance:0.5102151\n",
      "Welcome Praveen!\n",
      "distance:0.593511\n",
      "Welcome Rakshith!\n",
      "distance:0.44533187\n",
      "Welcome Rakshith!\n",
      "distance:0.51317203\n"
     ]
    }
   ],
   "source": [
    "students_set = detect_face_realtime(user_db, FRmodel, threshold = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rakshith'}\n"
     ]
    }
   ],
   "source": [
    "print(students_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pms.connect(host=\"localhost\",user=\"root\",password=\"\",db=\"attendance\")\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_date = time.strftime(\"%Y-%m-%d\")\n",
    "cur_time = time.strftime(\"%H:%M:%S\")\n",
    "cur_day = time.strftime(\"%w\")\n",
    "hour = int(cur_time.split(':')[0])\n",
    "hour_minus = hour-1\n",
    "prev_hour = cur_time.replace(str(hour),str(hour_minus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_query = (\"select subject_code from timetable where Day=\"+cur_day+\" and Time<=Time'\"+cur_time+\"' and Time>Time'\"+prev_hour+\"';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(sub_query)\n",
    "data,*_ = cursor.fetchall()\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_query = \"insert into attendance values('\"+cur_date+\"','\"+cur_time+\"',\"+str(data[-1])\n",
    "list_students_query = (\"select * from student\")\n",
    "cursor.execute(list_students_query)\n",
    "data = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert into attendance values('2018-11-25','16:19:47',10,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);\n"
     ]
    }
   ],
   "source": [
    "for a in data:\n",
    "    if a[1] in students_set:\n",
    "        insert_query += \",\"+str(1)\n",
    "    else:\n",
    "        insert_query += \",\"+str(0)\n",
    "insert_query += \");\"\n",
    "print(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(insert_query)\n",
    "connection.commit()\n",
    "data = cursor.fetchall()\n",
    "print(data)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
